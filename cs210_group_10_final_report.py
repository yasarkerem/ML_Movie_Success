# -*- coding: utf-8 -*-
"""CS210 Group-10 Final Report

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dvORkvfQxV-obsyjrdv1uyfvmQs_dpKq

# <font color = 'indianred' > **TMDB 5000 MOVIES - FINAL REPORT - GROUP ID: 10** </font>

## <font color = 'maroon' > ***Group Members:*** </font>

Ahmet Ali Sancaktaroğlu 26400 

Arda Sabancı 26978 

Mert Ekici 26772 

Utku Gürsoy 26572

Yaşar Kerem Cimilli 26428

# Introduction

It is subjective that whether a movie is successful or not. Some people think that a movie is successful if it has the most views, whereas others think that "Mainstream" is not always the best measure of success, since a very unsuccessful movie may be watched the most. Although the definiton of success in movie is subjective to every individual, “a successful movie” can be determined by such exploratory data analysis, by considering all parameters together. This Project aims to investigate a movie database, which contains 4803 movies, and try to find the effectiveness of parameters in terms of success.
</font>

# Problem Definition

Aim of this project is to extract reliable and applicable information which may give a tangible intuition to a film-maker about the possible nominative features of an upcoming film that would satisfy the expectations of people, by analyzing movies that have launched from the beginning of 20th century until today. We will try to formulize the key factors that what makes a movie successful and validate this formula with our dataset analysis. Overall, the project offers a solution that methodologically answers the questions “What are the qualities of an ideal film?” and “What makes a movie successful or award-winner?” considering specific parameters and time periods which are collected from the examined datasets
</font>

# Utilized Datasets

TMDB 5000 dataset contains 4083 fully acceptable (without Null values) films and 19 columns to examine those films.


**TMDB 5000 Movies:**

Link: https://www.kaggle.com/tmdb/tmdb-movie-metadata?select=tmdb_5000_movies.csv



**The columns in the tmdb_5000_movies.csv:** 

Column | Description
-----|:-------------:
budget | The budget of the movie
genre | The genre of the movie, Action, Comedy , Drama etc.
homepage | A link to the internet site of the movie.
id | Unique number which indicates a certain movie, it is the same with the first movie_id (credits.csv)
keywords | The related words to the movie.
original_language | The language in which the movie was made.
original_title | The title of the movie before translation or adaptation.
overview | A brief description of the movie.
popularity | A numeric quantity specifying the movie popularity.
production_companies | The production corporate of the movie.
production_countries | Countries that movie has produced in 
release_date | The date on which it was released.
revenue | The worldwide revenue obtained by the movie.
runtime | The running time of the movie in minutes.
status | "Released" or "Rumored" or "PostProduction".
tagline | Movie's tagline.
title | Title of the movie.
vote_average | Average rating that a movie 
vote_count | The count of votes that a movie has recieved.


**TMDB 5000 Credits:**

Link: https://www.kaggle.com/tmdb/tmdb-movie-metadata?select=tmdb_5000_credits.csv

**The columns in the tmdb_5000_credits.csv:**

Column | Description
-----|:-------------:
movie_id | Unique id for each movie.
cast | It contains all the names of cast members
crew | It contains names of Director, Editor, Composer, Writer etc.


With the help of these separators (columns), correlation between the films’ attributes can be seen. 
There are several observations which is emerged from films’ budgets, revenues, genres by the team.

**The Oscar Award, 1927 - 2020:**

Link: https://www.kaggle.com/unanimad/the-oscar-award?select=the_oscar_award.csv

Oscar awards dataset contains all the awards and their nominators since the beginning of the Academy (1928). 

**The columns in the_oscar_award.csv:**

Column | Description
-----|:-------------:
year_film | release year of the film
year_ceremony | year of the ceremony
ceremony | number of the ceremony (for the first Oscar Ceremony it is 1)
category | category of the Academy Award
name | name of the related subject (actor, director, producer...)
film | name of the film
winner | win or not by true-false
 
 
</font>

## <font color = 'lightseagreen' > **Exploratory Data Analysis** </font>

We have selected the useful columns (attributes) from the TMDB 5000 Movies dataset and categorized them into the four main titles. This method makes <font color = 'lightseagreen' > **`Exploratory Data Analysis`** </font> of the complex datasets easier. 

**Attributes**
---
<font color = 'red' > *cinematic*
<font color = 'blue' > *regional*
<font color = 'orange' > *financial*
<font color = 'purple' > *popularity*

**tmdb_5000_credits.csv**


* <font color = 'red' > title </font>
* <font color = 'red' > cast </font>
* <font color = 'red' > crew </font>


**tmdb_5000_movies.csv**

* <font color = 'blue' > original language </font>
* <font color = 'blue' > production country </font>
* <font color = 'blue' > spoken language </font>
* <font color = 'red' > genres </font>
* <font color = 'red' > runtime </font>
* <font color = 'red' > release date </font>
* <font color = 'red' > production company </font>
* <font color = 'orange' > budget </font>
* <font color = 'orange' > revenue </font>
* <font color = 'purple' > popularity </font>
* <font color = 'purple' > vote average </font>
* <font color = 'purple' > vote count </font>

</font></font></font></font>

---
### Data Exploration and Analysis Methods

Get to Know the Dataset

* Data cleaning <font color = 'darkgreen' > **`dropna`** </font>

* Genre distribution <font color = 'darkgreen' > **`barplot`** </font>

* Revenue - Budget distribution <font color = 'darkgreen' > **`horizontal barplot`** </font>

* Spoken Language Distribution <font color = 'darkgreen' > **`boxplot with and without outliers`** </font>

Oscar Award Dataset

* Oscar Winner Movies <font color = 'darkgreen' > **`horizontal barplot`** </font>

* Nominated Movies <font color = 'darkgreen' > **`horizontal barplot`** </font>

* Award Winner Actors, Actresses and Directors <font color = 'darkgreen' > **`barplot`** </font>

* Top 20 Award Winner Movies <font color = 'darkgreen' > **`df table`** </font>

Bivariate Analysis

* General Financial Bivariate Distriubtions <font color = 'darkgreen' > **`multiple scatter plots`** </font>

* Budget & Revenue by Genre <font color = 'darkgreen' > **`boxplot ascending`** </font>

* Revenue & Budget Top 20 <font color = 'darkgreen' > **`barplot overlapped`** </font>

* Release Year Distribution <font color = 'darkgreen' > **`boxplot`** </font>

* Popularity & Release Year by Genre <font color = 'darkgreen' > **`multiple scatter plots`** </font>

* Release Month Distribution <font color = 'darkgreen' > **`boxplot`** </font>

* Release Month & Revenue <font color = 'darkgreen' > **`monthly boxplot`** </font>

Weighted Average & Profit

* Vote Average vs Vote Counts <font color = 'darkgreen' > **`multiple horizontal barplots`** </font>

* Weighted Average and Estimated Profit Top 10's <font color = 'darkgreen' > **`df table`** </font>

* Weighted Average & Estimated Profit Distribution by Genre <font color = 'darkgreen' > **`scatter plot multitype`** </font>

* Billion Box Office <font color = 'darkgreen' > **`line plot`** </font>

##<font color = 'brown' >***mount your drive***</font> and <font color = 'purple' >***import the libraries***</font>
"""

from google.colab import drive
drive.mount("./drive")

path_prefix = "./drive/My Drive"

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from os.path import join

# %matplotlib inline

"""## ***read <font color = 'blue' >`csv files`</font> to <font color = 'blue' >`dataframes`</font>***


"""

fname1 = "tmdb_5000_movies.csv"
fname2 = "tmdb_5000_credits.csv"
fname3 = "the_oscar_award.csv"


df1 = pd.read_csv(join(path_prefix, fname1))
df2 = pd.read_csv(join(path_prefix, fname2))
df3 = pd.read_csv(join(path_prefix, fname3))
df1.info()
df1.head(2)

df2.info()
df2.head(2)

df3.info()
df3.head(2)

"""## Get to Know the Dataset

In this section, We are going to explore the dataset and going to display the distribution of some attributes to better understand the data which we are dealing with.

### NaN Values

In this notebook, we are not interested in all of the attributes. We need to make sure that there is no NaN value in the columns we are interested in.
"""

print(df1.isna().sum())
print(df2.isna().sum())
df1.dropna(axis="index", how="any", subset=["release_date",'runtime'],inplace=True)

"""**As seen above, there are missing datas in some of the selected columns: <font color='red'> release_date </font> and <font color='red'> runtime</font>.**

### Genre Distribution

In the `genre` column, movies are categorized into genres such as 
 <font color = 'violet' >*action*</font> , <font color = 'violet' >*thriller*</font>...

Before we find the number of movies of populer genres and display their percentages as a bar chart, we need to transform the genre column into a more readable version by using RegEx library.
"""

# regex library
import re
import ast

a = df1.loc[0,'genres']
print(a)

genre_pattern = r"([A-Z]\w+\s?\w+)"

def genres_rename(genre):
  
  a = re.findall(genre_pattern, genre)
  return a
   
df1["genres"] = df1["genres"].apply(genres_rename)

df1[["original_title","genres"]].head()

df_genre = pd.Series(sum([item for item in df1.genres], [])).value_counts(normalize=True,sort=True, ascending=True)
genres = df_genre.index
shares = df_genre.values*100
plt.figure(figsize=(15,7))
plt.barh(genres,shares,height=0.7,color = "lightblue")

# max(df_genre.values)

xlocs = np.linspace(0, max(shares),10)
ticks_labels = [f"{xloc:.0f}%" for xloc in xlocs]
plt.xticks(xlocs, ticks_labels)

plt.title(f"Movie Distribution by Genre")
plt.ylabel("Genre")
plt.xlabel("Share")
plt.grid(color="w",which="major",axis="x")
plt.show()

"""### Budget & Revenue Distribution

Columns `revenue` and `budget` stores the financial attributes of the movies. In order to understand the overall distribution and visualize the profitability, display the `revenue` and `budget` attributes as boxplots on the same figure.

However, the mean distributions are highly skewed. So, in order to better evaluate the distributions, we can remove the outliers with 1.5xIQR method.
"""

fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(20, 10))

ax1.set_title("Revenue & Budget Distributions")
df1.boxplot(['revenue',"budget"],ax =ax1)
ax1.set_ylabel("($) Millions")
ylocs1 = np.linspace(0, max(df1["revenue"].values), 10)
ticks_labels1 = [f"${yloc/10**6:.0f}M" for yloc in ylocs1]
ax1.set_yticks(ylocs1)
ax1.set_yticklabels(ticks_labels1)

Q1p = df1["revenue"].quantile(0.25)
Q3p = df1["revenue"].quantile(0.75)

Q1k = df1["budget"].quantile(0.25)
Q3k = df1["budget"].quantile(0.75)

IQRp = Q3p - Q1p
IQRk = Q3k - Q1k
filter = df1.loc[(df1['revenue'] >= Q1p-1.5*IQRp ) & (df1['revenue'] <= Q3p+1.5*IQRp)]
filter = filter.loc[(df1['budget'] >= Q1k-1.5*IQRk ) & (df1['budget'] <= Q3k+1.5*IQRk)]
ax2.set_title("Revenue & Budget Distributions - Outliers Removed")

filter.boxplot(["revenue","budget"],ax=ax2,color={'medians': 'orange','boxes':'black','whiskers':'black'})
ax2.set_ylabel("($) Millions")
ylocs2 = np.linspace(0, max(filter["revenue"].values), 10)
ticks_labels2 = [f"${yloc/10**6:.0f}M" for yloc in ylocs2]
ax2.set_yticks(ylocs2)
ax2.set_yticklabels(ticks_labels2)
ax1.set_xticklabels(['Revenue','Budget'])
ax2.set_xticklabels(['Revenue','Budget'])
ax2.grid(axis='x')

fig.suptitle("Revenue & Budget",fontsize=16)
plt.show()

"""### Spoken Language Distribution




"""

#language 
dfr = df1.copy()
eng = len(dfr.loc[dfr["original_language"] == "en"])
fr =  len(dfr.loc[dfr["original_language"] == "fr"])
oth = len(dfr.loc[dfr["original_language"] != "en"])
es = len(dfr.loc[dfr["original_language"] == "es"])
de = len(dfr.loc[dfr["original_language"] == "de"])
chi = len(dfr.loc[dfr["original_language"] == "zh"])

total = eng + fr+ de + es + chi

lst = [100*(eng/total), 100*(fr/total), 100*(es/total), 100*(de/total), 100*(chi/total), 100*(oth/total)]
lst2 = ["English", "French", "Spanish", "German", "Chinese", "Other Languages"]


plt.subplots(figsize=(10, 5))
plt.barh(lst2, lst, height=0.5)

xlocs = np.linspace(0, 100, 11)
ticks_labels = [f"{i:.0f}%" for i in xlocs]
plt.xticks(xlocs, ticks_labels)


plt.grid(axis='x', color='w')
plt.title("Original Language Distribution in Dataframe")
plt.xlabel("Language Share")
plt.ylabel("Language Types")
plt.show()

"""## Oscar Award Dataset

Now, we are going to explore Oscar Awards Dataset and analyze the results to detect award winner factors.

### Top 10 Nominated Movies

This graph shows the top 10 films in nomination stage even if they win or lose.
"""

df3.dropna(inplace=True, how='any', subset=["film","category", "name"])

def sep_movies(row):
  name = row['film']
  year = row["year_film"]
  return str(name) + " - " + str(year)

df3.dtypes
df3["uniquefilm"] = df3.apply(sep_movies,axis=1)


film_award_series = df3["uniquefilm"].value_counts(ascending=False)
film_award_series_top = film_award_series.head(10)

films = film_award_series_top.index
filmAward = film_award_series_top.values

fig, ax = plt.subplots(figsize=(20, 5))
plt.barh(films, filmAward)

plt.title("Top 10 Nomination Counts")
plt.xlabel("Nominations")
plt.show()

"""### Top 10 Oscar Awarded Movies

This chart visualizes the top 10 films which have the most Oscar Award counts.
"""

dfwin = df3[df3["winner"] == True]

film_award_series = dfwin["uniquefilm"].value_counts(ascending=False)
film_award_series_top = film_award_series.head(10)



films = film_award_series_top.index
filmAward = film_award_series_top.values

fig, ax = plt.subplots(figsize=(20, 5))
plt.barh(films, filmAward)

xticks = np.linspace(0,max(film_award_series_top.values),12)
plt.xticks(xticks)
plt.title("Top 10 Award Counts",fontsize=15)
plt.xlabel("Awards",fontsize=13)
plt.ylabel("Films",fontsize=13)

plt.show()

"""**These 2 graphics ease the examination process of award counts, nomination counts and winning rate of films.**

### Top 10 Award Winners (Actor, Actress, Director)

This section shows the top 10 actors/ actresses/ directors that have the most Academy award counts.
"""

dfc = df3.copy()
dfc_role = dfc[(dfc["category"] == "ACTOR") | (dfc["category"] == "ACTRESS") |
              (dfc["category"]=="ACTOR IN A LEADING ROLE") | (dfc["category"]=="ACTOR IN A SUPPORTING ROLE") |
              (dfc["category"]=="ACTRESS IN A LEADING ROLE") | (dfc["category"]=="ACTRESS IN A SUPPORTING ROLE") |
               (dfc["category"] == "DIRECTING")]


dfc_role = dfc_role[dfc_role.winner == True]

def ddlew(names):
  if "Daniel Day" in names:
    return "Daniel Day-Lewis"
  else:
    return names

dfc_role["name"] = dfc_role["name"].apply(ddlew)

player_award_series = dfc_role["name"].value_counts(ascending=False)

player_award_series_top = player_award_series.head(10)


player = player_award_series_top.index
playerAward = player_award_series_top.values


fig, ax = plt.subplots(figsize=(20, 5))
plt.bar(player, playerAward)

plt.yticks(np.linspace(0,4,5),[0,1,2,3,4])
plt.title("Award Winners",fontsize=18)
plt.xlabel("Winners",fontsize=13)
plt.ylabel("Counts",fontsize=13)
plt.show()

"""**Under the light of this chart, it can be understanded that there are some people who has individual credits independent from their just-one-film success.**

### Top 20 Award Winner Movies - Table

This list is going to be used with the main dataset to specify the award winner features.
"""

dfwin = dfwin[['uniquefilm']]
asd = dfwin.value_counts()
list20 = pd.DataFrame(asd.head(20))
list20

"""# Data Exploration

## Bivariate Analysis

In this section,  we are going to perform bivariate analysis on different attribute pairs.

### General Financial Bivariate Distributions

This part aims to visualize binary relationships of **`budget - revenue`**, **`budget - popularity`** and **`runtime - revenue`**.
"""

fig, (ax1,ax2,ax3) = plt.subplots(1, 3, figsize=(20, 6))

df_movie = df1[df1['revenue'] !=0]
df_movie = df_movie[df_movie['budget'] !=0]

revenue = df_movie['revenue']/10**6
budget = df_movie['budget']/10**6


#ax1
correlation = revenue.corr(budget)
print("Correlation Coefficient between Budget and Revenue:", correlation,"\n")

ylocs1 = np.linspace(0, max(revenue), 10)
ticks_labels = [f"${yloc:.0f}M" for yloc in ylocs1]
ax1.set_yticks(ylocs1)
ax1.set_yticklabels(ticks_labels)

xlocs1 = np.linspace(0, max(budget), 10)
ticks_labels = [f"${xloc:.0f}M" for xloc in xlocs1]
ax1.set_xticks(xlocs1)
ax1.set_xticklabels(ticks_labels, rotation = 90)

ax1.set_title("Budget vs Revenue")
ax1.set_xlabel("Budget")
ax1.set_ylabel("Revenue")
ax1.grid()
ax1.scatter(budget, revenue, color="b")

#ax2


popularity = df_movie['popularity']
budget = df_movie['budget']/10**6

correlation = budget.corr(popularity)
print("Correlation Coefficient between Budget and Popularity:", correlation,"\n")

xlocs1 = np.linspace(0, max(budget), 10)
ticks_labels = [f"${xloc:.0f}M" for xloc in xlocs1]
ax2.set_xticks(xlocs1)
ax2.set_xticklabels(ticks_labels, rotation = 90)

ax2.set_title("Budget vs Popularity")
ax2.set_xlabel("Budget")
ax2.set_ylabel("Popularity")
ax2.grid()
ax2.scatter(budget, popularity, color="b")

#ax3

revenue = df_movie['revenue']/10**6
runtime = df_movie['runtime']

correlation = revenue.corr(runtime)
print("Correlation Coefficient between Runtime and Revenue:", correlation,"\n")

ylocs1 = np.linspace(0, max(revenue), 10)
ticks_labels = [f"${yloc:.0f}M" for yloc in ylocs1]
ax3.set_yticks(ylocs1)
ax3.set_yticklabels(ticks_labels)

plt.title("Runtime vs Revenue")
plt.xlabel("Runtime")
plt.ylabel("Revenue")
plt.grid()
ax3.scatter(runtime, revenue, color="b")

fig.suptitle("Financial Bivariate Figures",fontsize=15)
plt.show()

"""- We tried to investigate whether there is a relation between budget and the renevue of the film. We found that the correlation coefficient between budget and revenue as **`0.7`** which indicates that there is a **moderate positive linear relationship** between budget and the renevue. Moreover, from the graph it can be seen that after a certain value if you increase your budget, amount of earned revenue does not increase in general.

- Also, we tried to investigate whether there is a correlation between budget and popularity. It turns out that the correlation coeffiecent between budget and popularity is **`0.4`**. There is a **moderate and almost weak linear relationship** between budget and popularity. Increase in budget does not increase popularity so much.

- Furthermore, in order to make more profit, short or long duration movies should not be prefered according to plots. The correlation coefficient between runtime and revenue is **`0.2`**. There is a **weak linear relationship** between runtime and revenue.

### Budget Distribution by Genre

The revenue and budget distributions are highly skewed as they can be observed in the previous question. With the generated charts, we see how the overall distribution is shaped with the help of boxplots. However, we can not observe how this distribution changes with respect to genres.

From the shared data dictionary, it can be seen that there are 20 different movie genres. Instead of putting all of the genres into account, we are going to focus on the most common and popular 11 genres to display the `budget` distribution for each of the selected genres.
"""

import statistics
np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning) 

g_bud = {}
for i in range(19, 8, -1):
  bud_list = []
  for index,row in df1.iterrows():
    if genres[i] in row['genres']:
      bud_list.append(row['budget'])
  g_bud[genres[i]]= bud_list    
      
med_dict= {}      
for i in g_bud.keys():
  med_dict[i] = statistics.median(g_bud[i])
  

sorted_values = sorted(med_dict.values())

sorted_dict = {}
for i in sorted_values:
    for k in g_bud.keys():
        if statistics.median(g_bud[k]) == i:
            sorted_dict[k] = g_bud[k]
            break


fig, ax = plt.subplots()
fig.set_size_inches(15,6)
ax.boxplot(sorted_dict.values())

ax.set_xticklabels(sorted_dict.keys())

ylocs1 = np.linspace(0, max(df1["budget"].values), 10)
ticks_labels1 = [f"${yloc/10**6:.0f}M" for yloc in ylocs1]
ax.set_yticks(ylocs1)
ax.set_yticklabels(ticks_labels1)
ax.set_xlabel("Genre",fontsize=15)
ax.set_ylabel("($) Millions",fontsize=13)
ax.set_title('Budget Distribution by Genre',fontsize=18)
ax.grid(b='both')
plt.show()

"""**Budget distribution by genre in ascending order above shows us which genres need more budget.**

### Revenue Distribution by Genre

Now, we are going to display the `revenue` distribution for each of the selected genres.
"""

import statistics

g_rev = {}
for i in range(19, 8, -1):
  rev_list = []
  for index,row in df1.iterrows():
    if genres[i] in row['genres']:
      rev_list.append(row['revenue'])
  g_rev[genres[i]]= rev_list    
      
med_dict2= {}      
for i in g_rev.keys():
  med_dict2[i] = statistics.median(g_rev[i])
  

sorted_values2 = sorted(med_dict2.values())

sorted_dict2 = {}
for i in sorted_values2:
    for k in g_rev.keys():
        if statistics.median(g_rev[k]) == i:
            sorted_dict2[k] = g_rev[k]
            break


fig, ax = plt.subplots()
fig.set_size_inches(18,10)
ax.boxplot(sorted_dict2.values())

ax.set_xticklabels(sorted_dict2.keys())

ylocs1 = np.linspace(0, max(df1["revenue"].values), 10)
ticks_labels1 = [f"${yloc/10**6:.0f}M" for yloc in ylocs1]
ax.set_yticks(ylocs1)
ax.set_yticklabels(ticks_labels1)
ax.set_xlabel("Genre",fontsize=15)
ax.set_ylabel("($) Millions",fontsize=13)
ax.set_title('Revenue Distribution by Genre',fontsize=18)
ax.grid(b='both')
plt.show()

"""Both `revenue` and `budget` distributions prove that **Adventure**, **Fantasy**, **Family**, **Action**, **Science Fiction** have the highest budgets and revenues.

### Revenue & Budget Comparison

In this part, we are going to show the budgets and revenues of the most expensive movies in the same bars of the bar plot.
"""

smalldf = df1[['original_title','revenue','budget']].sort_values(by='revenue',ascending=False).head(20)

smalldf
fig, ax = plt.subplots()
fig.set_size_inches(20,10)
rev = ax.bar(smalldf['original_title'], smalldf["revenue"],
                label="Revenue", zorder=1,color="skyblue")

bud = ax.bar(smalldf['original_title'], smalldf["budget"],
                 label="Budget", zorder=2,color="darkblue")
ax.set_xticklabels(smalldf['original_title'],rotation = 90)
ax.set_xlabel('Movie Title',fontsize=13)
ax.set_ylabel("($) Millions",fontsize=13)
ax.set_title('Top 20 - Revenue & Budget',fontsize=16)
ylocs1 = np.linspace(0, max(df1["revenue"].values), 10)
ticks_labels1 = [f"${yloc/10**6:.0f}M" for yloc in ylocs1]
ax.set_yticks(ylocs1)
ax.set_yticklabels(ticks_labels1)
ax.legend()
plt.show()

"""### Popularity vs. Release Year by Genres

In this part, we are going to display how the popularity changes as the release year changes for the selected movie genres. In the dataset, we have movies between 1916 and 2017. 

- First, we find the `release year` distribution for better understanding of the dataset.

- Then, group the dataframe by `genre` and `release year`, and find the popularity for each group.

"""

def year_column(release):
  return int(release[:4])

plt.figure(figsize=(13,8))
df1['release_year']=df1['release_date'].apply(year_column)
df1.boxplot(['release_year'])


plt.ylabel("Years")
plt.xticks(ticks=[1],labels=["Release Date"])
plt.title("Release Year Distribution",fontsize=14)
ax.grid(b='both')

plt.show()

# your code


g_pop = {}
for i in range(19, 8, -1):
  pop_list = []
  for index,row in df1.iterrows():
    if genres[i] in row['genres']:
      pop_list.append(row['popularity'])
  g_pop[genres[i]]= pop_list 

g_year = {}
for i in range(19, 8, -1):
  year_list = []
  for index,row in df1.iterrows():
    if genres[i] in row['genres']:
      year_list.append(row['release_year'])
  g_year[genres[i]]= year_list 

r = 3
c = 2
fig, axes = plt.subplots(r, c, figsize=(20, 12))

gens = ["Adventure", "Fantasy", "Family", "Action", "Science Fiction"]

for ind, genre in enumerate(gens):
  ax = axes[ind // c][ind % c]
  ax.plot(g_year[genre],g_pop[genre],'.')
  ax.set_title(genre, fontsize=15)
  ax.set_xlabel("Years")
  ax.set_ylabel("Popularity")
 
fig.delaxes(axes[2,1])


fig.suptitle("Popularity & Year", y=1.05, fontsize=20)
plt.tight_layout()
fig.show()

"""**The figure above shows the popularity change of the most expensive genres by years.**

### The Effect of Release Month on Revenue

**Distribution of Release Month**
"""

def month_column(release):
  return int(release[5:7])

plt.figure(figsize=(12,8))
df1['release_month']=df1['release_date'].apply(month_column)
df1.boxplot(['release_month'])


plt.ylabel("Month")
plt.xticks(ticks=[1],labels=["Release Month"])
plt.title("Release Month Distribution",fontsize=14)
ax.grid(b='both')

plt.show()

"""**Release Month & Revenue**"""

pd.options.mode.chained_assignment = None    

import calendar

df_movie['release_date'] = pd.to_datetime(df_movie['release_date'])

df_movie.loc[:,'release_month'] = df_movie['release_date'].dt.month

months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]

df_month = pd.DataFrame(columns=['revenue', 'release_month'])

for name, group in df_movie.groupby('release_month'):
  if name in months:
      df_month = df_month.append({'revenue': df_movie[df_movie['release_month'] == name]['revenue']/10**6, 'release_month': name}, ignore_index=True)

df_month = df_month.sort_values(by='release_month')

month_dict = dict(enumerate(calendar.month_abbr))
df_month.loc[:,'release_month'] = df_month['release_month'].map(month_dict)

fig, ax = plt.subplots(figsize = (17,5))
ax.grid()
ax.set_title('Revenue vs Release Month')
ax.set_xlabel('Release Month')
ax.set_ylabel('Revenue')

ylocs1 = np.linspace(0, max(df_movie['revenue']/10**6), 10)
ticks_labels = [f"${yloc:.0f}M" for yloc in ylocs1]
ax.set_yticks(ylocs1)
ax.set_yticklabels(ticks_labels)

ax_boxplot = ax.boxplot(df_month['revenue'], labels=df_month['release_month'])

"""**The months May-June-July and November-December have larger revenues compared to other months. These months are the most popular months for releasing movies, if you want to earn more profit.**

## Weighted Average & Profit

To estimate success scores of the movies in our dataset accurately, we need to analyze the <font color = 'orange' > **financial attributes** </font> and <font color = 'purple' > **popularity** </font> for each movie.

### Vote Average & Vote Count Comparison

The figure below shows  `vote averages` and the `vote counts` of the top 10 movies with the highest vote averages. It proves that without `vote count`, `vote average` is not a reliable parameter.
"""

#df1["vote_count"]= df1[df1["vote_count"] > 1000]

BAS = df1[["title","vote_average", "vote_count"]].sort_values(by="vote_average", ascending=True).tail(20)
#print(BAS)
BAS = BAS[BAS["vote_count"] > 1000]

#print(BAS)

fig, ax = plt.subplots(1,2, figsize=(25,10), sharey=True)
#fig.set_size_inches(10,10)
ax[0].barh(BAS['title'], BAS["vote_average"], label="Vote Average", zorder=1,color="coral")
ax[1].barh(BAS['title'], BAS["vote_count"], label="Vote Counts", zorder=1,color="lightblue")

ax[0].set_yticklabels(BAS['title'], fontsize=15, rotation=30)
ax[0].set_xlabel('Vote Averages out of 10',fontsize=20)
ax[0].set_ylabel("Movies",fontsize=20)
ax[0].set_title('Top Movies',fontsize=26)
ylocs0 = np.linspace(0, max(df1["vote_average"].values), 10)
ticks_labels0 = [f"{yloc:.0f}" for yloc in ylocs0]
ax[0].set_xticks(ylocs0)
ax[0].set_xticklabels(ticks_labels0)
ax[0].legend(loc="upper right", bbox_to_anchor=(2.43, 1),fontsize=13)

#ax[1].set_yticklabels(BAS['title'], rotation=30)
ax[1].set_xlabel('Number of Votes',fontsize=20)
#ax[1].set_ylabel("Movies",fontsize=13)
ax[1].set_title('Movies by their Vote Counts',fontsize=26)
ylocs1 = np.linspace(0, 10000, 10)
ticks_labels1 = [f"{yloc:.0f}" for yloc in ylocs1]
ax[1].set_xticks(ylocs1)
ax[1].set_xticklabels(ticks_labels1)
ax[1].legend(loc="upper right", bbox_to_anchor=(1.22,0.94), fontsize=13)


ax[0].grid(color="black", axis="x")
ax[1].grid(color="darkgray", axis="x")
plt.show()

"""### Weighted Average

In this dataset, movies have the attribute called `vote average` which represents the average of the votes that movies got. However, using only the `vote average` can be misleading to determine the movie success because there is another parameter which should be taking into consideration `vote count`.


![](https://trailerpark.weebly.com/uploads/8/8/5/5/8855465/7628808.png?371)


* W = weighted rating
* R = average rating for the movie as a number from 1 to 10 (vote_average)
* v = number of votes for the movie (vote_count)
* m = minimum votes required to be listed in the Top 250 (currently 25,000)
* C = the mean vote across the whole report

C <- mean(vote_average)

m <- quantile(vote_count, 0.75)

We are going to use IMDB's weighted rating formula into a new column called `weighted average`.
"""

# we already have v and R in our dataset but we need to calculate m and C.

m = df1["vote_count"].quantile(0.75)
C = df1["vote_average"].mean()

def weightavg(row):
  R = row["vote_average"]
  v = row["vote_count"]
  wavg = ((R*v)+(C*m))/(v+m)
  return wavg

df1["weighted_average"] = df1.apply(weightavg,axis=1)

top10wavg = df1.sort_values(by=["weighted_average"],ascending=False).head(10)
top10wavg.set_index("original_title",inplace=True)
display(top10wavg[['weighted_average']])

"""### $W = \frac{Rv + Cm}{v + m}$
***from the dataset C and m;***
### $C = 6.094 $
### $m = 737.25$
### $Cm = 6.094 \cdot 737.25$
### $Cm = 4492.8$
***weighted average with constants***
### $W = \frac{Rv + 4492.8}{v + 737.25}$

**This top 10 weighted average movie list consists with the IMDB's list as well.**

### Estimated Profit

As our other ingredient to formulate success, calculating the `profit` possible by subtracting the `budget` from `revenue` but it is not what exactly real profit is. Thus, we call the new column `estimated_profit`
"""

def estprof(row):
  return row['revenue']-row['budget']

df1['estimated_profit']=df1.apply(estprof,axis=1)
top10estp = df1.sort_values(by=["estimated_profit"],ascending=False).head(10)
top10estp.set_index("original_title",inplace=True)
display(top10estp[['estimated_profit']])

"""### Estimated Profit vs Weighted Average by Genre

And finally, we are going to check how `estimated profit` and `weighted average` change for different genres.

- Display a scatter plot in which estimated profits are encoded in x-axis and weighted averages in y-axis.

- At the end, each dot on the figure would represent a movie. Genres are highlighted with distinct color for separability.

"""

g_wa = {}
for i in range(19, 8, -1):
  wa_list = []
  for index,row in df1.iterrows():
    if genres[i] in row['genres']:
      wa_list.append(row['weighted_average'])
  g_wa[genres[i]]= wa_list 

g_ep = {}
for i in range(19, 8, -1):
  ep_list = []
  for index,row in df1.iterrows():
    if genres[i] in row['genres']:
      ep_list.append(row['estimated_profit'])
  g_ep[genres[i]]= ep_list 


plt.figure(figsize=(18,12))
for k in g_wa:
  plt.scatter(g_wa[k],g_ep[k],label=k,s=30)


plt.xticks(ticks=[5.0,5.5,6,6.5,7,7.5,8],labels=[5,5.5,6,6.5,7,7.5,8])
plt.yticks(ticks=[0.0*1e9,0.5*1e9,1.0*1e9,1.5*1e9,2.0*1e9,2.5*1e9],labels=["$0B","$0.5B","$1B","$1.5B","$2B","$2.5B"])
plt.ylabel('Estimated Profit in ($) Billions',fontsize=13)
plt.xlabel("Weighted Average (x/10)",fontsize=13)
plt.title("Weighted Average vs Estimated Profit by Movie Genre",fontsize=18)

plt.legend()
plt.show()

"""### Distribution of Weighted Average & Estimated Profit in Billion Box Office"""

def bil_club(bil):
    if bil > 1000000000:
        return "billionare"
  
df_b = df1.copy()
df_b["billionare_club"] = df_b["revenue"].apply(bil_club)

df_b = df_b.dropna(axis="index", how="any", subset=["billionare_club"])

tier_age_grp = df_b.groupby(by=["weighted_average", "billionare_club"])["estimated_profit"].mean().unstack().plot(grid=True, marker="D", figsize=(20,6), legend=True)
plt.title("Distribution of Weighted Average vs Estimated Profit in Billion Box Office", fontsize=15)
plt.xlabel("Weighted Average")
plt.ylabel("Estiamated Profit")
plt.xticks(ticks=[6.0,6.5,7.0,7.5,8.0],labels=[6,6.5,7,7.5,8])
plt.yticks(ticks=[0.75*1e9,1*1e9,1.25*1e9,1.5*1e9,1.75*1e9,2.0*1e9,2.25*1e9,2.5*1e9],labels=["$0.50B","$0.75B","$1.00B","$1.25B","$1.50B","$1.75B","$2.00B","$2.25B","$2.50B"])
plt.legend(loc="upper right", title="Weighted Average", title_fontsize=13)
plt.show()

"""# Succes Point

Success Point


$SP = 11W + 3\sqrt{ \frac{E}{m_E}}+5G+0.5\sqrt{P}$

$SP = Success \, Point$

$W = Weighted \, Average$

$E = Estimated \, Profit$

$m_E = Mean \, Estimated \, Profit$

$P = Popularity$

$G = Genre \, Point$ 

$ W < 8 \implies W =  W - \frac{10-W}{2}$ 



* Square root of estimated_profit over mean estimated profit can be maximum of 35 (Avatar).

* Genre point can be achieved if the movie has the popular genre of its release year (can be 1 or 0).

* Half of the square root of the popularity can be maximum of 7

* Success Points that excess 100 points are considered as 100 points.

* ***Depreciation*** : If the movie has W smaller than 8, then its weighted average decreases accordingly.

### Popular Genres of the Year

In this part, we are going to determine popular genres of the year and collect them into a dictionary. Then, turn them into genre point.
"""

years40 = df1["release_year"].value_counts(sort=True, ascending=True)[-40:]

popyear_group = df1.groupby(["release_year"]).agg({'popularity': ["max"] })

popyear_group = popyear_group.tail(40)
popyear_group=popyear_group.reset_index()
popyear_group.columns=popyear_group.columns.map('_'.join)
years40.index
popyear_group
pop_genres = {}
popyear_group.loc[0]["popularity_max"]
def pop_gendict(df):
  for i in range(40):
    a  = df.loc[(df["release_year"] == popyear_group.loc[i]["release_year_"]) & (df["popularity"] == popyear_group.loc[i]["popularity_max"])]
    a = a.reset_index()
    pop_genres[popyear_group.loc[i]["release_year_"]] = a.loc[0,"genres"]



pop_gendict(df1)

def gp_maker(row):
  gen = row["genres"]
  ye = row["release_year"]
  if ye < 1978:
    return 0;
  for i in gen:
    if i in pop_genres[ye]:
      return 1;
  return 0;

df1["genre_point"] = df1.apply(gp_maker,axis=1)

"""### Weight Adjustment

Below, we check the statistics of weighted_average, estimated_profit and popularity to adjust their weight in the "**Success Formula**".
"""

import math

m = df1["estimated_profit"].mean()
maxest = df1["estimated_profit"].max()
print(m,maxest)
int(math.sqrt(maxest/m))

s1 = df1["popularity"].max()
s2 = df1["popularity"].min()
s3 = df1["popularity"].mean()
print(s1,s2,s3)

int(math.sqrt(s1)/2)

as1 = df1["weighted_average"].max()
as2 = df1["weighted_average"].min()
as3 = df1["weighted_average"].mean()
print(as1,as2,as3)

"""### Top 50 Successful Movies

According to success points we formulated, these are the top 50 most successful movies.
"""

def sp_calculator(row):
  w = row["weighted_average"]
  e = row["estimated_profit"]
  g = row["genre_point"]
  p = row["popularity"]
  if e < 0:
    e = 0;
  if w < 8:
    w = w - (10-w)/2
  sp = 11*w + 3*(math.sqrt(e/m)) + g*5 + 0.5*(math.sqrt(p))
  if sp > 100:
    return int(100)
  elif sp<0:
    return int(0)
  else:
    return int(sp)

df1["success_point"] = df1.apply(sp_calculator,axis=1)

top50success = df1.sort_values(by=["success_point"],ascending=False).head(50)
top50success = top50success.reset_index()
display(top50success[["original_title","success_point","genre_point","weighted_average","estimated_profit"]])

"""## Creating the Label

In this section, we are going to create a new column called "successful" with the values 1(yes) or 0(no) for the Machine Learning Part (going to be used as a label for some methods).
"""

ss1 = df1["success_point"].max()
ss2 = df1["success_point"].min()
ss3 = df1["success_point"].mean()
ss4 = df1["success_point"].std()
print(ss1,ss2,ss3,ss4)

"""With the statistics above, we decided to assign movies with success_point higher than;

$SP > \frac{mean+0.5(max+min)}{2}+1.5 \sigma $

$SP > 72 $

"""

def successfinder(sp):
  if sp > 72:
    return 1
  else:
    return 0

df1["successful"] = df1["success_point"].apply(successfinder)

df1["successful"].mean()

"""## Top 20 Award Winner's of the Oscar Dataset (Are they successful?)

Now, we are going to compare the Oscar datasent with ours by checking the top 20 award winner movies if they are succesful or not.
"""

nlist20 = [i[0][:-7] for i in list20.index]

for i in nlist20:
  if i in df1.title.values:
    x = df1[df1["title"] == i]
    if x.successful.values == 1:
      print(i," is a successful movie.")

"""From the results above we can conclude that our success determination method coincides some of the top 20 award winner movies. However, we used many parameters elaborately to formulate success and it focuses some other aspects of a movie as well.

# Hypothesis Test

## Success Point Test

In the Hypothesis Test section, we are going to test our formulated  parameter `success_point` with the `vote_average` since it is not used directly in the Success Point formula and it is one of the acceptable ways to evaluate a movies success. Our hypothesis is being successful as a movie increases the vote average.
"""

from scipy import stats
from scipy import special
import matplotlib.style
import matplotlib as mpl
mpl.style.use('default')
#mpl.style.use("ggplot")

"""### Statistics

First, we need to get statistics of the vote_average column such as mean and standard deviation.
"""

popm = df1["vote_average"].mean()
popstd = df1.vote_average.std()
popmin = df1.vote_average.min()
plt.boxplot(df1.vote_average)

print("mean of the vote_average column: ",popm)
print("std of the vote_average column: ",popstd)
print("minimum of the vote_average column: ",popmin)

top50m = top50success.vote_average.mean()
top50std =top50success.vote_average.std()

print("mean of the vote_average column (top50): ",top50m)
print("std of the vote_average column (top50): ",top50std)

"""Since we know the vote_average standard deviation and the sample size is greater than 30, best method to use in hypothesis test is z-distribution.

### Normal Distribution

Now, using the stats above we are going to turn vote_average into a normal distribution.
"""

# mean and std values from the sample
n = 50 #top 50 successful movies will be used for hypothesis test 
mean = popm
std = popstd / np.sqrt(n)
offset = 4*std

# the x-axis ticks of the plot
# generates 100 equally separated ticks
x = np.linspace(mean - offset, mean + offset, n)

# probability density function
# of the given normal dist.
y = stats.norm.pdf(x, mean, std)

plt.figure(figsize=(10, 5))
plt.plot(x,y)
# put grids on the figure
plt.grid("both")
plt.xlabel("x")
plt.ylabel("Normal Distribution")
plt.fill_between(x, y, alpha=0.3, color="b")
plt.title(f"Normal Dist. with Mean {mean}, Std {std}")
plt.show()

"""### Z Score

$\Large{Z = \frac{\bar{x}_{sample} - \mu_{sampling \, dist.}}{\sigma_{sampling \, dist.}}}$

We can use the **z-scores** and find how many standard deviations away the sample mean is from the population mean. 
"""

# calculating the z-score
z_score = (top50m - popm) / std

print("z-score: {}".format(z_score))

prob = 2*stats.norm.cdf(-z_score)

print(f"probability: {prob}")

probalm = 1 - stats.norm.cdf(z_score)

print("probability is almost ",probalm)

"""It is almost impossible to have a mean of 7.882 for vote_average (top 50 successful movies) with the normal distribution. Therefore, our success parameter works well.

## Genre Point - Popularity Test

Next, we are going to test this hypothesis: Genre Point causes the higher popularity.

### Statistics

First, we need to get statistics of the popularity column such as mean and standard deviation.
"""

pom = df1["popularity"].mean()
postd = df1.popularity.std()
pomin = df1.popularity.min()
plt.boxplot(df1.popularity)

print("mean of the popularity column: ",pom)
print("std of the popularity column: ",postd)
print("minimum of the popularity column: ",pomin)

genrem = df1[df1["genre_point"] == 1].popularity.mean()
genrestd =df1[df1["genre_point"] == 1].popularity.std()

print("mean of the popularity column (genrepoint=1): ",genrem)
print("std of the popularity column (genrepoint=1): ",genrestd)

"""Since we know the population standard deviation and the sample size is greater than 30, best method to use in hypothesis test is z-distribution.

### Normal Distribution

Now, using the stats above we are going to turn popularity into a normal distribution.
"""

# mean and std values from the sample
n = df1[df1["genre_point"] == 1].genre_point.sum()#top 50 successful movies will be used for hypothesis test 
mean = pom
std = postd / np.sqrt(n)
offset = 4*std

# the x-axis ticks of the plot
# generates 100 equally separated ticks
x = np.linspace(mean - offset, mean + offset, n)

# probability density function
# of the given normal dist.
y = stats.norm.pdf(x, mean, std)

plt.figure(figsize=(10, 5))
plt.plot(x,y)
# put grids on the figure
plt.grid("both")
plt.xlabel("x")
plt.ylabel("Normal Distribution")
plt.fill_between(x, y, alpha=0.3, color="b")
plt.title(f"Normal Dist. with Mean {mean}, Std {std}")
plt.show()

"""### Z Score

$\Large{Z = \frac{\bar{x}_{sample} - \mu_{sampling \, dist.}}{\sigma_{sampling \, dist.}}}$

We can use the **z-scores** and find how many standard deviations away the sample mean is from the population mean. 
"""

# calculating the z-score
z_score = (genrem - pom) / std

print("z-score: {}".format(z_score))

prob = 2*stats.norm.cdf(-z_score)

print(f"probability: {prob}")

print("probability is almost 0")

"""It is almost impossible to have a mean of 26.94 for popularity (movies with genre points) with the normal distribution by chance. Therefore, genre points increases popularity.

# Machine Learning Methods

## k-Nearest Neighbours

In this part, we are going to use `kNN` method to predict genre_point by analyzing release_year and weighted_average.
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
  
X = df1[["release_year","weighted_average"]]
y = df1["genre_point"]

# 80% for training and 20% for testing-validation
X_traink, X_remainingk, y_traink, y_remainingk = train_test_split(X, y, test_size=0.20, random_state=42)
# 10% validation, 10% test
X_testk, X_valk, y_testk, y_valk = train_test_split(X_remainingk, y_remainingk, test_size=0.50, random_state=42)

"""### Best k Value"""

from sklearn.metrics import accuracy_score
ks = list(range(1,15))

accuracies = []
for k in ks:
  model = KNeighborsClassifier(k, metric="euclidean")
  model.fit(X_traink, y_traink)
  y_predk = model.predict(X_valk) 
  accuracies.append(accuracy_score(y_valk,y_predk))

plt.plot(ks,accuracies,'*-')
plt.show()

"""Best k value to use in our model seems to be 13.

### Visualization
"""

# let's visualize its location on the chart
plt.scatter( df1["release_year"],df1["weighted_average"],c=df1["genre_point"], s=20)

plt.title("Genre point by Year and Average")
plt.show()

from matplotlib.colors import ListedColormap
from ipywidgets import interact

def visualize_knn(k):

 model = KNeighborsClassifier(k, metric="euclidean")
 model.fit(X_traink, y_traink)

 cmap_light = ListedColormap(['cyan', 'cornflowerblue'])
 cmap_bold = ListedColormap([ 'c', 'darkblue'])


 xx, yy = np.meshgrid(X_testk.release_year.values[:300], X_testk.weighted_average.values[:300])
 labels = model.predict(np.hstack([xx.reshape(-1,1), yy.reshape(-1,1)]))
 # print(xx,yy,labels)
 labels = labels.reshape(xx.shape)
 plt.figure()
 plt.pcolormesh(xx, yy, labels, cmap=cmap_light)
 plt.scatter(df1["release_year"],df1["weighted_average"],c=df1.genre_point, cmap=cmap_bold, edgecolor='k', s=20)
 plt.xlim(xx.min(), xx.max())
 plt.ylim(yy.min(), yy.max())
 plt.show() 

interact(visualize_knn, k=(1,14))

"""As seen from the interactive k-NN figure above, movies show tendency to embrace popular genres after 2000 and at the first years of this movement it helped movies to get high weighted averages.(Please use the best k value which is 13.)

## Random Forest

We take "successful" as label but which attributes is our machine learning method going to be trained on?

* We are going to drop popularity, estimated_profit, weighted_average, genre_point, success_point and successful because they have already been used in our secret formula.

* We are going to drop all object typed columns.

* We are going to drop id since it is irrelevant.
"""

print(df1.dtypes[df1.dtypes.values != object])

dfrf = df1[["budget","runtime","release_year","release_month"]]
labelrf = df1["successful"]

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

y = labelrf
X = dfrf

X_train, X_remaining, y_train, y_remaining = train_test_split(X, y, test_size=0.20, random_state=42)

X_test, X_val, y_test, y_val = train_test_split(X_remaining, y_remaining, test_size=0.50, random_state=42)

model_rf = RandomForestClassifier(criterion="entropy")
model_rf.fit(X_train, y_train)

predval = model_rf.predict(X_val)
print("the accuracy on the validation set: ",accuracy_score(y_val,predval))

"""### Feature Importances"""

from matplotlib import cm

feature_list = list(X.columns)
importances = list(model_rf.feature_importances_)
feature_importance = [(feature, importance) for feature, importance in zip(feature_list, importances)]
feature_importances = sorted(feature_importance, key = lambda x: x[1], reverse = True)

sortedfeatures = sorted(feature_importances[:25],key = lambda x: x[1],reverse = False)


plt.figure(figsize=(14,8))
plt.grid("both")
plt.barh([val[0] for val in sortedfeatures],[val[1] for val in sortedfeatures],color = cm.gnuplot(np.linspace(0.9,0.3,4)))
plt.xlabel("Feature Importance")
plt.title("Feature Importances of the Initial Model")

"""The bar chart shows the importance measurements of the attributes from the initial model.

### Grid Search - Best Parameters
"""

from sklearn.model_selection import GridSearchCV

param_grid = {'n_estimators': [100,200,300], 'min_samples_split': [2,4]}
rf_grid = GridSearchCV(estimator = model_rf, param_grid = param_grid)
rf_grid.fit(X_train,y_train)
print(rf_grid.best_params_)

grid_preds = rf_grid.predict(X_val)
print(accuracy_score(y_val,grid_preds))

"""**<font color="skyblue"> Accuracy score has increased from 0.95208 to 0.95625 with the best hyperparameter values that Grid Search method determined:  </font>**

<font color="violet"> **`min_samples_split: 4, n_estimators: 200`** </font>

### Confusion Matrix
"""

from sklearn.metrics import confusion_matrix
import seaborn as sns

grid_testpreds = rf_grid.predict(X_test)

print(accuracy_score(y_test,grid_testpreds))

sns.set()
mat = confusion_matrix(y_test, grid_testpreds)
sns.heatmap(mat, annot=True, fmt='d')
plt.xlabel('predicted label')
plt.ylabel('true label');

"""<font color="cadetblue"> As the confusion matrix above shows; </font> 
* true positives (TP): 2
* true negatives (TN): 458
* false positives (FP): 1
* false negatives (FN): 19


$Accuracy = \frac{TP+TN}{total} = 0.9583 $

$MisclassificationRate = \frac{FP+FN}{total} = 0.0416 $

$Recall = \frac{TP}{actual yes} = 0.0952 $

$ Precision = \frac{TP}{predicted yes} = 0.66 $

$F1Score = \frac{{2}\cdot{recall}\cdot{precision}}{recall+precision} = 0.1663 $


**These statistics shows that our model has high accuracy but recall and precision numbers might be misleading since the actual yes and predicted yes numbers are extremely low because of the small rate of succesful movies in our dataset (0.04).**

## Decision Tree
In this part, we tried to implement desicion tree machine learning method in our dataset in order to evaluate how well Success Point parameters predicts a film is a succesful film or not and tried to find the answer of the question "What are the inputs that makes a succesful film?". 


**Tree Visualization:**


Visualize the resulting tree model with the help of sklearn.
"""

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(max_depth=3, criterion="entropy")

model.fit(X_train, y_train)

"""Visualize the resulting tree in text format."""

from sklearn.tree import export_text

print(export_text(model, feature_names=list(X.columns)))

"""Plot the tree itself."""

from sklearn.tree import plot_tree
import matplotlib as mpl

#plt.style.use('dark_background')
mpl.rcParams['text.color'] = 'black'
fig = plt.figure(figsize=(15, 10),facecolor='k')

plot_tree(model, feature_names=X.columns.values, class_names=["Unsuccessful", "Successful"], filled=True);

"""From the results of decision tree method, it can be seen that in order a film to be a succesful film, its budget has to be more than 122500000 dolars. As we observed in the data exploration where we investigated relation between Relase Month vs Revenue, the relaese month of the film has to be in November or December. Moreover, its runtime has to be greater than 116 minutes.

**Decision Tree:**
"""

from sklearn import tree

model_dt = tree.DecisionTreeClassifier() 
model_dt.fit(X_train, y_train)

dt_predictions = model_dt.predict(X_test)
dt_acc = accuracy_score(y_test, dt_predictions)

print("Decision Tree Accuracy:"+str(dt_acc))

"""## Logistic Regression 
In order to test how well our formula for succes point predicts succesful films, we applied another machine learning method which is Logistic Regression. We tried to compute how `budget`,`runtime`,`release year`,`release month` are important for Succes Point and how these paramaters effect the desicion of our model to predict a movie succesful movie.

Data exploration:
"""

import seaborn as sns

print (df1["successful"].value_counts(), "\n")

sns.countplot(x="successful", data=df1, palette="hls")
plt.show()

count_no_succ = len(df1[df1['successful']==0])
count_succ = len(df1[df1['successful']==1])
print("Percentage of no successful:", (count_no_succ/(count_no_succ+count_succ))*100)
print("Percentage of successful:", (count_succ/(count_no_succ+count_succ))*100)

"""In our dataset, according to our formula there are 4574 unsuccesful and 226 succesful movies. Moreover, according to our Succes Point  %4.7 of the films  are succesful and %95.2 are unsuccesful.

Create a Logistic Regression classifier object and fit model on the train set
"""

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()

logreg.fit(X_train,y_train)

"""Predicting the test set results and calculating the accuracy"""

y_pred=logreg.predict(X_test)
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))

"""### Confusion Matrix

*Confusion* Matrix:
"""

from sklearn.metrics import confusion_matrix
from sklearn import metrics

confusion_matrix = metrics.confusion_matrix(y_test, y_pred)
print(confusion_matrix)

"""A confusion matrix is used to evaluate the performance of a classification mode as a table. Diagonal values represent accurate predictions, while non-diagonal elements are inaccurate predictions. We have 459+3=462 correct predictions and 18+0=18 incorrect predictions.

Confusion Matrix using Heatmap:
"""

sns.set()
sns.heatmap(pd.DataFrame(confusion_matrix),cmap="YlGnBu" , annot=True, fmt='g')
plt.xlabel('Predicted label')
plt.ylabel('Actual label');
plt.title('Confusion matrix')

"""Confusion Matrix Evaluation Metrics:"""

from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

"""**TP:** predicted "yes" and actually "yes"

**TN:** predicted "no" and actually "no"

**FP:** predicted "yes", actually "no" 

**FN:** predicted "no", actually "yes" 

**Accuracy:** Overall, how often is the classifier correct?

* (TP+TN)/total = (3+459)/480 = 0.9625

**Misclassification Rate:** Overall, how often is it wrong?

* (FP+FN)/total = (0+18)/480 = 0.0375

**Recall:** When it's actually yes, how often does it predict yes?

* TP/actual yes = 3/21 = 0.14285714

**Precision:** When it predicts yes, how often is it correct?

* TP/predicted yes = 3/3 = 1

**F1-score:**

* 2xrecallxprecision/(recall+precision) = 2x0.14285714x1/(0.14285714+1) = 0,25

**These statistics shows that our model has high accuracy and misclassification rate but recall and precision numbers might be misleading since the actual yes and predicted yes numbers are extremely low because of the small rate of succesful movies in our dataset.**

### Word Cloud

These are Word clouds are showing the most frequent keywords used to describe a movie in our dataset and the latter shows the most frequent keywords used to describe movies which have been founded successful according to our results.
"""

from wordcloud import WordCloud, STOPWORDS

df1_copy = df1.copy()
pattern = "([a-z]\w+\s?\w+)"

def keywords_rename(keyword):
  a = re.findall(pattern, keyword)
  return a
   
def plot_cloud(wordcloud):
    plt.figure(figsize=(20, 10))
    plt.imshow(wordcloud) 
    plt.axis("off");

df1_copy["keywords"] = df1_copy["keywords"].apply(keywords_rename)

text = " ".join("\t".join(review) for review in df1_copy.keywords)
text1 = " ".join("\t".join(review) for review in df1_copy.sort_values(by=["success_point"],ascending=False).head(50).keywords)

#WORDCLOUD OF ALL MOVIES
wordcloud = WordCloud(width = 2000, height = 1500, random_state=1, background_color='black', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(text)
plot_cloud(wordcloud)

#TOP 50 Movies WordCloud
wordcloud1 = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(text1)
plot_cloud(wordcloud1)

"""## Discussion & Results

In Machine Learning Part, we have used 4 different methods: k-NN, Random Forest, Decision Tree and Logistic Regression. 

Method | Accuracy
-----|:-------------:
k-NN | 0.65
Random Forest | 0.95
Decision Tree | 0.92
Logistic Regression | 0.96

* In k-NN part we used release_year and weighted_average to predict genre_point but data was too complicated to use k-NN. Accordingly, the accuracy was low but still we could gather some information from this method.

   * Result: Movies show tendency to embrace popular genres after 2000 and at the first years of this movement it helped movies to get high weighted averages.


* In Random Forest method, we used "budget", "runtime", "release_year", "release_month" as attributes and "successful" as label. By using feature importances we visualize and compare the importances of the attributes in terms of their contribution to making a movie successful which is the aim of our project. Also, we used grid search to find best parameters for our Random Forest method. In confusion matrix, the accuracy of the method was satisfying. On the other hand, recall and precision values were not reliable since the rate of successful movies among all of the movies in the dataset is too small (0.04). True predictions and actual trues are relatively small, as well.

   * Result: Budget, runtime, release_year and release_month has an effect on a movie's success by decreasing order.

* In Desicion Tree method, we tried to test our formula and observe how our formula works to correctly predicts succesful films. In order to observe and interpret easily the results of the model, we kept the depth of the tree small. The advantage of using decision tree compared to other models was it has no tendency to underfit so it gave a trustable result to us. We used this method because success point is a catogerical data and desicion tree works well with categorical data. For downside, this method is inaccurate compared to other machine learning methods that we applied which can be see from its accuracy score.

   * Result: In order to a film be a succesful film, its budget has to be more than 122500000 dollars. Relaese month of the film has to be in November or December. Its runtime has to be greater than 116 minutes. These results coincides with the results in the data exploration part about the relation between various parameters and succesful films.  This shows that our formula works very well.


* In the Logistic Regression method, we used "budget", "runtime", "release_year", "release_month" as keywords and "successful" as label. We tried to evaluate how well our formula works by classifying succesful film as "1" and unsuccesful film as "0". We get the best accuracy (0.9625) in this model compared to other models. This shows that in terms of accuracy, Logistic Regression gives the best accuracy. As in the random forest method, recall and precision values were not reliable since the rate of successful movies among all of the movies in the dataset is too small.

  * Result: In the models we have used, except for kNN, each of model's accuracy score is higher than 0.9 and very close to 1. This shows that our formula predicts a succesful film succesfully. Although high accuracy of the model may seem suspicious for overfitting, accuracy of our test data was also high which means there is no overfitting; models work correctly work as expected. If there was overfitting in the model, accuracy of the test results would be significantly low.

# Conclusion
Measure of success of a movie is an undeniably subjective judgement that may differ among individuals. The storyline, character development, recording techniques or a message aimed to be given of the movie or the emotions of audience and these kind of parameters are also important to determine whether a movie is really successful or not, and we also agree on that. However, in order to obtain a palpable or a solid understanding of a measure of success, we have analyzed 5000 movies which includes mostly numeric and interpretable features about those movies, such as revenue, release date, budget, actors, votes etc. During our analysis, we used various machine learning models and techniques to test our hypotheses of success. Training ML models with two different datasets gave us a more comprehensive understanding of that which movies are successful as a result of our quantitative analysis. 

As a conclusion, we accomplished to formularize the key factors that makes a movie successful and validate this formula with our Machine Learning models, we determined the most important features of a movie in order to be considered as successful or award-winner, and showed the qualities of a successful movie with respect to our datasets. However, there was not any extractable information about the cause people’s votes that may enable us to understand why they voted in such a way and what were they thinking while they are voting. Therefore we could not make any analysis on that in our models and leave it as an open-ended question by excluding it from our metric. Nonetheless, we provide sufficient analysis for a film-maker who wants to shoot a successful movie with respect to a justified metric.

# Future Work

Merging the Oscar database with TMDB5000 database in order to see Academy Award’s effect on a movie’s success. Understanding the mechanism how pre-awarded subjects effect the movie's popularity and what are the award-winner factors for the movie.

Since the conclusion is showed that this project seems reliable and accurate, there could be several projects that can use this project as a template. Firstly, the formula of success rate can be enhanced under the light of new databases with different films and attributes. Also vote features can be categorized such as directing, music, actor performance, etc. Thus, more accurate and reliable success rate formula can be created. Secondly, the correlation between the success rate and keywords can be examined more deeply. Thirdly, an application can be developed which offers movies according to their success rate and, same or different genre films for the user through their response. Fourthly, this project can be extended to contain TV series. Parameters of the success rate can be modified accordingly series’ corresponding features. Finally, this project can be used to make successful films and maybe even series with using success rate and necessary parameters.

</font>

# Work Division

**Ahmet - Kerem - Utku**:
- They are assigned to <font color = 'teal' > work on the TMDB5000 database </font> and <font color = 'teal' > individuate the effects that makes a movie successful in terms of financial features </font> by sketching graphs in order to see is there any correlation between those investigated effects. Worked on the implementation of the machine learning methods and interpretation of the results of these methods.   



**Arda**:
- He is assigned to <font color = 'teal' > work on The Oscar “Award 1927- 2020” dataset and distiguish relatable data from it </font> in order to find Academy award’s effectiveness in movie’s success. Worked on the implementation of the machine learning methods and interpretation of the results of these methods. 

**Mert**:
- He is also assigned to work on the TMDB5000 database and <font color = 'teal' > distinguish the effects that makes a movie successful </font> by sketching graphs in order to see is there any correlation between those investigated effects. He
worked on the  <font color = 'teal' > formulated parameters like weighted average, estimated profit and Succes Point</font>. Worked on the implementation of the machine learning methods and interpretation of the results of these methods. He also was <font color = 'teal' > the editor of the report and the notebook </font>.
"""